{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Necessary Set-up"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, Embedding\n",
    "from tensorflow.python.ops.losses import losses\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "\n",
    "tsne_model = TSNE(learning_rate=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 12551609130865645008\n, name: \"/device:GPU:0\"\ndevice_type: \"GPU\"\nmemory_limit: 3179663360\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 8448255061708723872\nphysical_device_desc: \"device: 0, name: Quadro K2200, pci bus id: 0000:02:00.0, compute capability: 5.0\"\n]\nNum GPUs Available:  1\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "tf.__version__\n",
    "print(device_lib.list_local_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Embedder Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model Architecture, Losses and Accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### The Encoder Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def build_encoder(embedding_size):\n",
    "  model = Sequential()\n",
    "  \n",
    "  # The first encoder layer\n",
    "  model.add(Dense(embedding_size*4, activation='relu'))\n",
    "  \n",
    "  # The second encoder layer\n",
    "  model.add(Dense(embedding_size*2, activation='relu'))\n",
    "  \n",
    "  # The output layer\n",
    "  model.add(Dense(embedding_size, activation='relu'))\n",
    "  \n",
    "  return model\n",
    "  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### The Decoder Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def build_decoder(embedding_size, output_size):\n",
    "  model = Sequential()\n",
    "  \n",
    "  # The first decoder layer\n",
    "  model.add(Dense(embedding_size*2, activation='relu'))\n",
    "  \n",
    "  # The secod decoder layer\n",
    "  model.add(Dense(embedding_size*4, activation='relu'))\n",
    "  \n",
    "  # The third decoder layer\n",
    "  model.add(Dense(output_size, activation='sigmoid'))\n",
    "  \n",
    "  return model\n",
    "  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Putting together the Ecnoder and Decoder into an AutoEncoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def build_ae(encoder, decoder, output_size):\n",
    "  \n",
    "  input_tensor = Input(shape=(output_size,))\n",
    "  embeddings = encoder(input_tensor)\n",
    "  reconstructions = decoder(embeddings)\n",
    "\n",
    "  auto_encoder = Model(input_tensor, reconstructions)\n",
    "  \n",
    "  return auto_encoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### The Auto-encoder Loss and Acuraccy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def recon_loss(x, x_hat):\n",
    "  \n",
    "  return tf.reduce_sum(tf.keras.losses.binary_crossentropy(x, x_hat))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def first_order_loss(X, Z):\n",
    "    \n",
    "    X = tf.cast(X, tf.float32)\n",
    "    Z= tf.cast(Z, tf.float32)\n",
    "    \n",
    "    D = tf.linalg.diag(tf.reduce_sum(X,1))\n",
    "    L = D - X ## L is laplation-matriX\n",
    "    \n",
    "    return 2*tf.linalg.trace(tf.matmul(tf.matmul(tf.transpose(Z),L),Z))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def ae_adversarial_loss(X, Z, x, x_hat, d_z0, d_z1, first_order, alpha):\n",
    "  \n",
    "  # Recon loss \n",
    "  reccon_loss = recon_loss(x, x_hat)\n",
    "  f1_loss = first_order_loss(X, Z)\n",
    "  \n",
    "  if first_order =='with_f1':\n",
    "      reccon_loss += alpha * f1_loss\n",
    "  \n",
    "  ### Loss 2 -> Same as the loss of the generator\n",
    "  adversarial_loss = tf.reduce_sum(tf.keras.losses.binary_crossentropy(tf.ones_like(d_z0), d_z0)) + \\\n",
    "                     tf.reduce_sum(tf.keras.losses.binary_crossentropy(tf.zeros_like(d_z1), d_z1))\n",
    "  \n",
    "  return  reccon_loss  + 10 * adversarial_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def ae_accuracy(x, x_hat):\n",
    "  round_x_hat = tf.round(x_hat)\n",
    "  return tf.reduce_mean(tf.cast(tf.equal(x, round_x_hat), tf.float32))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pretraining the Embedder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def pretrain_step_embd(X, x, encoder, decoder, auto_encoder, pre_optimizer, first_order, alpha):\n",
    "\n",
    "  with tf.GradientTape() as pre_tape:\n",
    "    z = encoder(x, training=True)\n",
    "    x_hat = decoder(z, training=True)\n",
    "    \n",
    "    Z = encoder(X, training=True)\n",
    "    \n",
    "    pre_loss = recon_loss(x, x_hat) \n",
    "    \n",
    "    if first_order == 'with_f1':\n",
    "        pre_loss += alpha * first_order_loss(X, Z)\n",
    "    \n",
    "    \n",
    "  pre_gradients = pre_tape.gradient(pre_loss, auto_encoder.trainable_variables)\n",
    "  pre_optimizer.apply_gradients(zip(pre_gradients, auto_encoder.trainable_variables))\n",
    "  \n",
    "  pre_acc = ae_accuracy(x, x_hat)\n",
    "  \n",
    "  return tf.reduce_mean(pre_loss), pre_acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def pretrain_embd(X, idxs, encoder, decoder, auto_encoder, pre_optimizer, first_order, alpha):\n",
    "  np.random.shuffle(idxs)\n",
    "  PRE_EPOCHS = 300\n",
    "  Batch_size = 50\n",
    "  \n",
    "  for epoch in range(PRE_EPOCHS):\n",
    "    \n",
    "    epoch_losses = []\n",
    "    epoch_acc = []\n",
    "    \n",
    "    for batch_idx in range(0, len(idxs), Batch_size):\n",
    "    \n",
    "      selected_idxs = idxs[batch_idx : batch_idx + Batch_size]\n",
    "      adjacency_batch = X[selected_idxs, :]\n",
    "\n",
    "      loss, accuracy= pretrain_step_embd(X, tf.cast(adjacency_batch, tf.float32), encoder, decoder, auto_encoder, pre_optimizer, first_order, alpha)\n",
    "      \n",
    "      epoch_losses.append(loss)\n",
    "      epoch_acc.append(accuracy)\n",
    "      \n",
    "#     if epoch % 50 == 0:\n",
    "#        print(f\"Loss is {np.array(epoch_losses).mean()} and accuracy is {np.array(epoch_acc).mean()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Discriminator Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model Architecture"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def build_discriminator(embedding_size):\n",
    "  model = Sequential()\n",
    "  \n",
    "  # The input layer\n",
    "  model.add(Input(shape=(embedding_size,)))\n",
    "  \n",
    "  # The first hidden layer\n",
    "  model.add(Dense(25, activation='relu'))\n",
    "  model.add(Dropout(0.25))\n",
    "  \n",
    "  # The second layer\n",
    "  model.add(Dense(15, activation='relu'))\n",
    "  model.add(Dropout(0.20))\n",
    "\n",
    "  # The third layer\n",
    "  model.add(Dense(6, activation='relu'))\n",
    "  model.add(Dropout(0.20))\n",
    "  \n",
    "  model.add(Dense(1, activation = 'sigmoid'))\n",
    "  \n",
    "  return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def disc_loss_function(d_z0, d_z1):\n",
    "   \n",
    "  loss_zero = tf.keras.losses.binary_crossentropy(tf.zeros_like(d_z0), d_z0) \n",
    "  loss_one = tf.keras.losses.binary_crossentropy(tf.ones_like(d_z1), d_z1)\n",
    "\n",
    "  return tf.cast(loss_zero, tf.float32) + tf.cast(loss_one, tf.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Joint Training in a step"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def train_step(X, x0, x1, encoder, decoder, auto_encoder, discriminator, ae_optimizer, disc_optimizer, first_order, alpha):\n",
    "  with tf.GradientTape() as ae_tape, tf.GradientTape() as disc_tape:\n",
    "    \n",
    "    z0 = encoder(x0, training=True)\n",
    "    z1 = encoder(x1, training=True)\n",
    "    \n",
    "    Z = encoder(X, training=True)\n",
    "    \n",
    "    d_z0 = discriminator(z0, training=True)\n",
    "    d_z1 = discriminator(z1, training=True)\n",
    "    \n",
    "    x0_hat = decoder(z0, training=True)\n",
    "    x1_hat = decoder(z1, training = True)\n",
    "    \n",
    "       \n",
    "    ae_loss = ae_adversarial_loss(X, Z, tf.concat([x0, x1], 0), tf.concat([x0_hat, x1_hat], 0), d_z0, d_z1, first_order, alpha)\n",
    "    disc_loss = disc_loss_function(d_z0, d_z1)\n",
    "    \n",
    "    \n",
    "  gradients_ae = ae_tape.gradient(ae_loss, auto_encoder.trainable_variables)\n",
    "  gradients_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "  \n",
    "  ae_optimizer.apply_gradients(zip(gradients_ae, auto_encoder.trainable_variables))\n",
    "  disc_optimizer.apply_gradients(zip(gradients_disc, discriminator.trainable_variables))\n",
    "  \n",
    "  ae_acc = ae_accuracy(tf.concat([x0, x1], 0), tf.concat([x0_hat, x1_hat], 0))\n",
    "  \n",
    "  return tf.reduce_mean(ae_loss), ae_acc, tf.reduce_mean(disc_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pretrain Step for Discriminator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def pretrain_step_disc(x0, x1, encoder, discriminator, disc_pre_optimizer):\n",
    "  \n",
    "  z0 = encoder(x0)\n",
    "  z1 = encoder(x1)\n",
    "  \n",
    "  with tf.GradientTape() as disc_tape_sep:\n",
    "    \n",
    "    d_z0= discriminator(z0, training=True)\n",
    "    d_z1 = discriminator(z1, training=True)\n",
    "       \n",
    "    disc_loss = disc_loss_function(d_z0, d_z1)\n",
    "\n",
    "    \n",
    "  gradients_disc = disc_tape_sep.gradient(disc_loss, discriminator.trainable_variables)\n",
    "  disc_pre_optimizer.apply_gradients(zip(gradients_disc, discriminator.trainable_variables))\n",
    "  \n",
    "  return tf.reduce_mean(disc_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pretraining the discriminator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def pretrain_disc(X, idxs_zeros, idxs_ones, encoder, discriminator, disc_pre_optimizer):\n",
    "  \n",
    "  EPOCHS = 40\n",
    "\n",
    "  np.random.shuffle(idxs_zeros)\n",
    "  np.random.shuffle(idxs_ones)\n",
    "  Batch_size = 50\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    for batch_idx in range(0, len(idxs_ones), Batch_size):\n",
    "    \n",
    "      selected_zeros = idxs_zeros[batch_idx : batch_idx + Batch_size]\n",
    "      selected_ones = idxs_ones[batch_idx : batch_idx + Batch_size]\n",
    "      \n",
    "      x0 = X[selected_zeros]\n",
    "      x1 = X[selected_ones]\n",
    "\n",
    "      pretrain_step_disc(x0, x1, encoder, discriminator, disc_pre_optimizer)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The Train Loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def adversarial_train(X, idxs_zeros, idxs_ones, encoder, decoder, auto_encoder, discriminator, ae_optimizer, disc_optimizer, first_order, alpha):\n",
    "    EPOCHS = 500\n",
    "    \n",
    "    np.random.shuffle(idxs_zeros)\n",
    "    np.random.shuffle(idxs_ones)\n",
    "    \n",
    "    Batch_size = 50\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for batch_idx in range(0, len(idxs_ones), Batch_size):\n",
    "        \n",
    "          selected_zeros = idxs_zeros[batch_idx : batch_idx + Batch_size]\n",
    "          selected_ones = idxs_ones[batch_idx : batch_idx + Batch_size]\n",
    "          \n",
    "          x0 = X[selected_zeros]\n",
    "          x1 = X[selected_ones]\n",
    "        \n",
    "          ### Joint Training\n",
    "          train_step(X, tf.cast(x0, tf.float32), tf.cast(x1, tf.float32), encoder, decoder,\n",
    "                                                     auto_encoder, discriminator, ae_optimizer, disc_optimizer, first_order, alpha)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Piciking the Seeds Using Embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def get_seeds(N_CLUS, embedding, nodes, labels, nodes_zero, nodes_one, strategy, n_seeds):\n",
    "    '''\n",
    "    stratgey can be random, nearest, fair, re-cluster, fair_re-cluster\n",
    "    '''\n",
    "    \n",
    "    model = KMeans(n_clusters=N_CLUS)\n",
    "    model.fit(embedding)\n",
    "    \n",
    "    cluster_number = model.labels_  \n",
    "    centers = model.cluster_centers_\n",
    "    \n",
    "    seed_ids = [[] for i in range(N_CLUS)]   \n",
    "    \n",
    "    for i in range(N_CLUS):\n",
    "    \n",
    "        if strategy == 'nearest':\n",
    "          sorted_distance = np.array(sorted([[np.sqrt(np.sum(np.power(centers[i] - embedding[j], 2))), j] for j in range(len(embedding)) if i == cluster_number[j]]))\n",
    "          seed_ids[i].extend(list(sorted_distance[:n_seeds, 1]))\n",
    "        \n",
    "        \n",
    "        elif strategy == 're-cluster':\n",
    "          temp = []\n",
    "          sorted_distance = np.array(sorted([[np.sqrt(np.sum(np.power(centers[i] - embedding[j], 2))), j] for j in range(len(embedding)) if i == cluster_number[j]]))\n",
    "          temp.extend(list(sorted_distance[:n_seeds, 1]))\n",
    "          \n",
    "          portion_zero = 0\n",
    "          portion_one = 0\n",
    "          \n",
    "          for num in temp:\n",
    "            if num in nodes_zero:\n",
    "              portion_zero += 1\n",
    "            elif num in nodes_one:\n",
    "              portion_one += 1\n",
    "          \n",
    "          zero_in_clus = embedding[np.logical_and(cluster_number == i, labels == 0)]\n",
    "          zero_inds = nodes[np.logical_and(cluster_number == i, labels == 0)]\n",
    "          \n",
    "          one_in_clus = embedding[np.logical_and(cluster_number == i, labels == 1)]\n",
    "          one_inds = nodes[np.logical_and(cluster_number == i, labels == 1)]   \n",
    "          \n",
    "          added_to_zero = 0\n",
    "          if len(zero_in_clus) != 0:\n",
    "              model_on_zero = KMeans(n_clusters=1)\n",
    "              model_on_zero.fit(zero_in_clus)\n",
    "              center_zero = model_on_zero.cluster_centers_\n",
    "              \n",
    "              sorted_distance_zero = np.array(sorted([[np.sqrt(np.sum(np.power(center_zero - zero_in_clus[j], 2))), j] for j in range(len(zero_in_clus))]))\n",
    "              seed_ids[i].extend([zero_inds[int(i)] for i in sorted_distance_zero[:portion_zero, 1]])\n",
    "              \n",
    "              added_to_zero = len(seed_ids[i])\n",
    "              assert added_to_zero == portion_zero\n",
    "               \n",
    "          added_to_one = 0\n",
    "          if len(one_in_clus) != 0:\n",
    "              model_on_one = KMeans(n_clusters=1)\n",
    "              model_on_one.fit(one_in_clus)\n",
    "              center_one = model_on_one.cluster_centers_\n",
    "              \n",
    "              sorted_distance_one = np.array(sorted([[np.sqrt(np.sum(np.power(center_one - one_in_clus[j], 2))), j] for j in range(len(one_in_clus))]))\n",
    "              seed_ids[i].extend([one_inds[int(i)] for i in sorted_distance_one[:portion_one, 1]])\n",
    "              \n",
    "              added_to_one = len(seed_ids[i]) - added_to_zero\n",
    "              assert added_to_one == portion_one\n",
    "              \n",
    "          assert n_seeds == added_to_zero + added_to_one\n",
    "          assert len(seed_ids[i]) == n_seeds\n",
    "    \n",
    "    return np.reshape(seed_ids, newshape=(-1, ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The IC algorithm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def IC(G, seeds, imp_prob, recover_prob = 0, remove = 0):\n",
    "  \n",
    "    impressed = []\n",
    "    removed = []\n",
    "    front = list(seeds[:])\n",
    "    \n",
    "    while front:\n",
    "        impressed.extend(front)\n",
    "        impressed = np.array(impressed)\n",
    "    \n",
    "        if recover_prob != 0:\n",
    "    \n",
    "            random_draws = np.random.uniform(size=len(impressed))\n",
    "    \n",
    "            if remove:\n",
    "                removed.extend(impressed[random_draws < recover_prob])\n",
    "                removed = list(set(removed))\n",
    "    \n",
    "            impressed = impressed[random_draws >= recover_prob]\n",
    "              \n",
    "        impressed = list(impressed)\n",
    "        new_front = []\n",
    "    \n",
    "        for node in front:\n",
    "    \n",
    "            neighbours = list(G.neighbors(node))\n",
    "    \n",
    "            for neigh in neighbours:\n",
    "    \n",
    "                expr_prob = np.random.uniform(size=1)[0]\n",
    "                if expr_prob < imp_prob and not (neigh in impressed) and not (neigh in new_front) and not (neigh in removed):\n",
    "                    new_front.append(neigh)\n",
    "    \n",
    "        front = new_front[:]\n",
    "     \n",
    "    impressed = np.reshape(np.array(impressed), newshape=(-1,))\n",
    "    \n",
    "    return impressed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Repeated IC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def repeated_IC(G, nodes_zero, nodes_one, seeds, seeds_type, n_expr, imp_prob, recover_prob = 0, remove = 0):\n",
    "  zeros_count = []\n",
    "  ones_count = []\n",
    "  total_count = []\n",
    "  \n",
    "  for i in range(n_expr):\n",
    "    impressed = IC(G, seeds, imp_prob, recover_prob = recover_prob, remove = remove)\n",
    "    total_count.append(len(impressed))\n",
    "    \n",
    "    count_zeros = 0\n",
    "    count_ones = 0\n",
    "    \n",
    "    for imp in impressed:\n",
    "      if imp in nodes_zero:\n",
    "        count_zeros += 1\n",
    "      elif imp in nodes_one:\n",
    "        count_ones += 1\n",
    "    \n",
    "    zeros_count.append(count_zeros)\n",
    "    ones_count.append(count_ones)\n",
    "    \n",
    "  total_imp = np.round(np.mean(total_count), 2)\n",
    "  total_fraction = np.round(total_imp / len(G.nodes()), 3)\n",
    "  \n",
    "  fraction_zero = np.round(np.mean(zeros_count) / len(nodes_zero), 3)\n",
    "  fraction_one = np.round(np.mean(ones_count) / len(nodes_one), 3)\n",
    "  \n",
    "  return total_imp, total_fraction, fraction_zero, fraction_one\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the real graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def get_graph_real():\n",
    "  graph_df = pd.read_csv('edges.txt', sep=\"\\t\", header=None)\n",
    "  graph_df.columns = ['s', 't']\n",
    "  \n",
    "  attr_df = pd.read_csv('attr.txt', sep=\"\\t\", header=None)\n",
    "  attr_df.columns = ['id', 'College', 'Age', 'Major']\n",
    "  \n",
    "  edges = []\n",
    "\n",
    "  for index, row in graph_df.iterrows():\n",
    "    edge_cur = (row.s, row.t)\n",
    "\n",
    "    edges.append(edge_cur)\n",
    "    \n",
    "  input_G = nx.from_edgelist(edges)\n",
    "  \n",
    "  extra_nodes = []\n",
    "  for index, row in attr_df.iterrows():\n",
    "    if row.Age > 20:\n",
    "      extra_nodes.append(row.id)\n",
    "  \n",
    "  unfrozen_G = nx.Graph(input_G)\n",
    "  \n",
    "  for node in input_G.nodes():\n",
    "    if node in extra_nodes:\n",
    "      unfrozen_G.remove_node(node)\n",
    "  \n",
    "  X = nx.to_numpy_matrix(unfrozen_G)\n",
    "  G = nx.from_numpy_matrix(X)\n",
    "\n",
    "  return G, X, unfrozen_G"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def get_nodes_labels_real(input_G):\n",
    "  data = pd.read_csv('attr.txt', sep=\"\\t\", header=None)\n",
    "  data.columns = ['id', 'College', 'Age', 'Major']\n",
    "\n",
    "  dict_labels = {}\n",
    "\n",
    "  for index, row in data.iterrows():\n",
    "    if row.Age < 20:\n",
    "      dict_labels[row.id] = 0\n",
    "    elif row.Age == 20:\n",
    "      dict_labels[row.id] = 1\n",
    "    else:\n",
    "      dict_labels[row.id] = 2\n",
    "\n",
    "  labels = []\n",
    "  for node in list(input_G.nodes()):\n",
    "    labels.append(dict_labels[node])\n",
    "\n",
    "  # Remember that whenever you want do logical operation on a sequence, that sequence should be numpy array\n",
    "  labels = np.array(labels)\n",
    "\n",
    "  nodes = np.arange(len(input_G.nodes()))\n",
    "\n",
    "  nodes_zero = nodes[labels == 0]\n",
    "  nodes_one = nodes[labels == 1]\n",
    "\n",
    "  return nodes_zero, nodes_one, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def get_idxs(n, nodes_zero, nodes_one):\n",
    "  \n",
    "  diff_size = np.abs(len(nodes_one) - len(nodes_zero))\n",
    "  \n",
    "  idxs_zeros = nodes_zero[:]\n",
    "  idxs_ones = nodes_one[:]\n",
    "  rep_policy = False\n",
    "  \n",
    "  if len(nodes_zero) < len(nodes_one):\n",
    "    if diff_size > len(nodes_zero):\n",
    "      rep_policy = True\n",
    "    zero_draws = np.random.choice(nodes_zero,size=diff_size, replace=rep_policy)\n",
    "    idxs_zeros = np.concatenate((idxs_zeros, zero_draws))\n",
    "  \n",
    "  elif len(nodes_zero) > len(nodes_one):\n",
    "    if diff_size > len(nodes_one):\n",
    "      rep_policy = True\n",
    "    one_draws = np.random.choice(nodes_one,size=diff_size, replace=rep_policy)\n",
    "    idxs_ones = np.concatenate((idxs_ones, one_draws))\n",
    "  \n",
    "  assert len(idxs_zeros) == len(idxs_ones)\n",
    "  \n",
    "  return np.arange(n), idxs_zeros, idxs_ones\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def print_edges(G, nodes_zero, nodes_one):\n",
    "  zero_edges = 0\n",
    "  one_edges = 0\n",
    "  accross_edges = 0\n",
    "\n",
    "  for (v1, v2) in G.edges():\n",
    "    if v1 in nodes_zero and v2 in nodes_zero:\n",
    "      zero_edges += 1\n",
    "    elif v1 in nodes_one and v2 in nodes_one:\n",
    "      one_edges += 1\n",
    "    elif v1 in nodes_one and v2 in nodes_zero:\n",
    "      accross_edges += 1\n",
    "    elif v1 in nodes_zero and v2 in nodes_one:\n",
    "      accross_edges += 1\n",
    "\n",
    "  print(f\" edges in zero community: {zero_edges}\")\n",
    "  print(f\" edges in one community: {one_edges}\")\n",
    "  print(f\" edges across communities: {accross_edges}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Running the experiments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      " edges in zero community: 513\n edges in one community: 7441\n edges across communities: 1706\n",
      "WARNING:tensorflow:Layer sequential is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\n",
      "pre-training done.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "embedding_size = 30\n",
    "\n",
    "# Can get `with_f1` or `without_f1`\n",
    "first_order_imp = 'no_f1'\n",
    "alpha = 0.05\n",
    "\n",
    "# 1. Creating the Graph and Getting the Adj Matrix\n",
    "G, X, input_G = get_graph_real()\n",
    "n = len(G.nodes())\n",
    "\n",
    "# 2. Getting seperate lists for seperate communities and the label for each community\n",
    "nodes_zero, nodes_one, labels = get_nodes_labels_real(input_G)\n",
    "\n",
    "print_edges(G, nodes_zero, nodes_one)\n",
    "\n",
    "# 3. Getting the idxs suitable for training.\n",
    "idxs, idxs_zeros, idxs_ones = get_idxs(n, nodes_zero, nodes_one)\n",
    "\n",
    "# 4. Creating the Embedder\n",
    "encoder = build_encoder(embedding_size)\n",
    "decoder = build_decoder(embedding_size, n)\n",
    "auto_encoder = build_ae(encoder, decoder, n)\n",
    "\n",
    "# 5. Creating the Discriminator\n",
    "discriminator = build_discriminator(embedding_size)\n",
    "\n",
    "# 6. Pretraining the Embedder and the Discriminator\n",
    "pre_optimizer_embd = tf.keras.optimizers.Adam()\n",
    "pre_optimizer_disc = tf.keras.optimizers.Adam()\n",
    "\n",
    "pretrain_embd(X, idxs, encoder, decoder, auto_encoder, pre_optimizer_embd, first_order_imp, alpha)\n",
    "pretrain_disc(X, idxs_zeros, idxs_ones, encoder, discriminator, pre_optimizer_disc)\n",
    "# print('6')\n",
    "\n",
    "# # 6-1. Get the pretrain-embeddings\n",
    "pre_embds = encoder(X)\n",
    "print('pre-training done.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "adversarial training done.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 7. Adversarial Training\n",
    "ae_optimizer = tf.keras.optimizers.Adam()\n",
    "disc_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "adversarial_train(X, idxs_zeros, idxs_ones, encoder, decoder, auto_encoder, discriminator, ae_optimizer, disc_optimizer, first_order_imp, alpha)\n",
    "\n",
    "#7-1. Getting the fair-embeddings\n",
    "fair_embds = encoder(X)\n",
    "\n",
    "print('adversarial training done.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Experiments settings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "N_CLUSs = [4]\n",
    "\n",
    "n_seedss = [1, 2, 3, 4, 5, 6, 7, 8, 10]\n",
    "# n_seedss = [8]\n",
    "\n",
    "# Methods for getting the seeds can be nearest or re-cluster\n",
    "strategy = 're-cluster'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[30, 4, 1, 7.21, 0.016, 0.015, 0.017, 8.44, 0.019, 0.004, 0.023, \"'re-cluster'\"]\n",
      "[30, 4, 2, 14.67, 0.033, 0.03, 0.034, 16.63, 0.038, 0.018, 0.043, \"'re-cluster'\"]\n",
      "[30, 4, 3, 21.07, 0.048, 0.066, 0.043, 22.35, 0.051, 0.034, 0.055, \"'re-cluster'\"]\n",
      "[30, 4, 4, 28.98, 0.066, 0.082, 0.061, 27.98, 0.063, 0.057, 0.065, \"'re-cluster'\"]\n",
      "[30, 4, 5, 36.01, 0.082, 0.12, 0.071, 35.41, 0.08, 0.082, 0.08, \"'re-cluster'\"]\n",
      "[30, 4, 6, 43.42, 0.098, 0.112, 0.095, 42.04, 0.095, 0.107, 0.092, \"'re-cluster'\"]\n",
      "[30, 4, 7, 50.98, 0.116, 0.138, 0.109, 48.69, 0.11, 0.142, 0.102, \"'re-cluster'\"]\n",
      "[30, 4, 8, 55.01, 0.125, 0.17, 0.112, 53.01, 0.12, 0.152, 0.111, \"'re-cluster'\"]\n",
      "[30, 4, 10, 68.95, 0.156, 0.199, 0.144, 67.81, 0.154, 0.173, 0.148, \"'re-cluster'\"]\n[[30, 4, 1, 7.21, 0.016, 0.015, 0.017, 8.44, 0.019, 0.004, 0.023, 're-cluster'],\n[30, 4, 2, 14.67, 0.033, 0.03, 0.034, 16.63, 0.038, 0.018, 0.043, 're-cluster'],\n[30, 4, 3, 21.07, 0.048, 0.066, 0.043, 22.35, 0.051, 0.034, 0.055, 're-cluster'],\n[30, 4, 4, 28.98, 0.066, 0.082, 0.061, 27.98, 0.063, 0.057, 0.065, 're-cluster'],\n[30, 4, 5, 36.01, 0.082, 0.12, 0.071, 35.41, 0.08, 0.082, 0.08, 're-cluster'],\n[30, 4, 6, 43.42, 0.098, 0.112, 0.095, 42.04, 0.095, 0.107, 0.092, 're-cluster'],\n[30, 4, 7, 50.98, 0.116, 0.138, 0.109, 48.69, 0.11, 0.142, 0.102, 're-cluster'],\n[30, 4, 8, 55.01, 0.125, 0.17, 0.112, 53.01, 0.12, 0.152, 0.111, 're-cluster'],\n[30, 4, 10, 68.95, 0.156, 0.199, 0.144, 67.81, 0.154, 0.173, 0.148, 're-cluster']]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "rows = '['\n",
    "first = True\n",
    "\n",
    "\n",
    "for N_CLUS in N_CLUSs:\n",
    "  for n_seeds in n_seedss:\n",
    "    if first:\n",
    "        first = False\n",
    "    else:\n",
    "        rows += ',\\n'\n",
    "    #8. Getting the seeds for the embeddings and baselines\n",
    "    fair_seeds = get_seeds(N_CLUS, fair_embds, idxs, labels, nodes_zero, nodes_one, strategy, n_seeds)\n",
    "    pre_seeds = get_seeds(N_CLUS, pre_embds, idxs, labels, nodes_zero, nodes_one, strategy, n_seeds)\n",
    "    \n",
    "    #9. Getting the final results\n",
    "    total_fair, fair_frac, zero_fair, one_fair = repeated_IC(G, nodes_zero, nodes_one, fair_seeds, 'fair', 2000, 0.01)\n",
    "    total_pre, pre_frac, zero_pre, one_pre = repeated_IC(G, nodes_zero, nodes_one, pre_seeds, 'pre', 2000, 0.01)    \n",
    "    \n",
    "    #10. Building the current row and adding it to the rows.\n",
    "    row =[ embedding_size ,  N_CLUS,  n_seeds, total_fair, fair_frac, zero_fair, one_fair,  total_pre ,  pre_frac ,  zero_pre ,  one_pre, '\\'' + strategy + '\\'']\n",
    "    print(row)\n",
    " \n",
    "    rows += '[' + ', '.join(map(str, row)) + ']'\n",
    "    \n",
    "rows += ']'\n",
    "\n",
    "\n",
    "print(rows)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Saved\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('embeddings_6.pickle', 'wb') as f:\n",
    "    pickle.dump([G, embedding_size, fair_embds, pre_embds, idxs, labels, nodes_zero, nodes_one], f)\n",
    "\n",
    "print('Saved')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}