{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Necessary Set-up"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, Embedding\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "tsne_model \u003d TSNE(learning_rate\u003d100)"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 8161728494386800531\n]\n"
          ],
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m\u003cipython-input-2-c1c5681230bb\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_local_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----\u003e 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Num GPUs Available: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\u0027GPU\u0027\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: module \u0027tensorflow\u0027 has no attribute \u0027config\u0027"
          ],
          "ename": "AttributeError",
          "evalue": "module \u0027tensorflow\u0027 has no attribute \u0027config\u0027",
          "output_type": "error"
        }
      ],
      "source": [
        "tf.__version__\n",
        "print(device_lib.list_local_devices())\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices(\u0027GPU\u0027)))\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Embedder Model"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Architecture, Losses and Accuracy"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### The Encoder Model"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": [
        "def build_encoder(embedding_size):\n",
        "    model \u003d Sequential()\n",
        "\n",
        "    # The first encoder layer\n",
        "    model.add(Dense(embedding_size * 4, activation\u003d\u0027relu\u0027))\n",
        "\n",
        "    # The second encoder layer\n",
        "    model.add(Dense(embedding_size * 2, activation\u003d\u0027relu\u0027))\n",
        "\n",
        "    # The output layer\n",
        "    model.add(Dense(embedding_size, activation\u003d\u0027relu\u0027))\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### The Decoder Model"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": [
        "def build_decoder(embedding_size, output_size):\n",
        "    model \u003d Sequential()\n",
        "\n",
        "    # The first decoder layer\n",
        "    model.add(Dense(embedding_size * 2, activation\u003d\u0027relu\u0027))\n",
        "\n",
        "    # The secod decoder layer\n",
        "    model.add(Dense(embedding_size * 4, activation\u003d\u0027relu\u0027))\n",
        "\n",
        "    # The third decoder layer\n",
        "    model.add(Dense(output_size, activation\u003d\u0027sigmoid\u0027))\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Putting together the Ecnoder and Decoder into an AutoEncoder"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": [
        "def build_ae(encoder, decoder, output_size):\n",
        "    input_tensor \u003d Input(shape\u003d(output_size,))\n",
        "    embeddings \u003d encoder(input_tensor)\n",
        "    reconstructions \u003d decoder(embeddings)\n",
        "\n",
        "    auto_encoder \u003d Model(input_tensor, reconstructions)\n",
        "\n",
        "    return auto_encoder\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### The Auto-encoder Loss and Acuraccy"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": [
        "def recon_loss(x, x_hat):\n",
        "    return tf.reduce_sum(tf.keras.losses.binary_crossentropy(x, x_hat))\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": [
        "def first_order_loss(X, Z):\n",
        "    X \u003d tf.cast(X, tf.float32)\n",
        "    Z \u003d tf.cast(Z, tf.float32)\n",
        "\n",
        "    D \u003d tf.linalg.diag(tf.reduce_sum(X, 1))\n",
        "    L \u003d D - X  ## L is laplation-matriX\n",
        "\n",
        "    return 2 * tf.linalg.trace(tf.matmul(tf.matmul(tf.transpose(Z), L), Z))\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "source": [
        "def ae_adversarial_loss(x1, x1_hat, d_z10, d_z11, x2, x2_hat, d_z20, d_z21):\n",
        "    # Recon loss\n",
        "    reccon_loss_1 \u003d recon_loss(x1, x1_hat)\n",
        "    reccon_loss_2 \u003d recon_loss(x2, x2_hat)\n",
        "    \n",
        "    reccon_loss \u003d reccon_loss_1 + reccon_loss_2\n",
        "    \n",
        "    ### Loss 2 -\u003e Same as the loss of the generator\n",
        "    adversarial_loss_1 \u003d tf.reduce_sum(tf.keras.losses.binary_crossentropy(tf.ones_like(d_z10), d_z10)) + \\\n",
        "                       tf.reduce_sum(tf.keras.losses.binary_crossentropy(tf.zeros_like(d_z11), d_z11))\n",
        "    \n",
        "    adversarial_loss_2 \u003d tf.reduce_sum(tf.keras.losses.binary_crossentropy(tf.ones_like(d_z20), d_z20)) + \\\n",
        "                   tf.reduce_sum(tf.keras.losses.binary_crossentropy(tf.zeros_like(d_z21), d_z21))\n",
        "    \n",
        "    adversarial_loss \u003d adversarial_loss_1 + adversarial_loss_2\n",
        "\n",
        "    return reccon_loss + 2 * adversarial_loss\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "source": [
        "def ae_accuracy(x, x_hat):\n",
        "    round_x_hat \u003d tf.round(x_hat)\n",
        "    return tf.reduce_mean(tf.cast(tf.equal(x, round_x_hat), tf.float32))\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pretraining the Embedder"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "source": [
        "def pretrain_step_embd(X, x, encoder, decoder, auto_encoder, pre_optimizer, first_order, alpha):\n",
        "    with tf.GradientTape() as pre_tape:\n",
        "        z \u003d encoder(x, training\u003dTrue)\n",
        "        x_hat \u003d decoder(z, training\u003dTrue)\n",
        "\n",
        "        Z \u003d encoder(X, training\u003dTrue)\n",
        "\n",
        "        pre_loss \u003d recon_loss(x, x_hat)\n",
        "\n",
        "        if first_order \u003d\u003d \u0027with_f1\u0027:\n",
        "            pre_loss +\u003d alpha * first_order_loss(X, Z)\n",
        "\n",
        "    pre_gradients \u003d pre_tape.gradient(pre_loss, auto_encoder.trainable_variables)\n",
        "    pre_optimizer.apply_gradients(zip(pre_gradients, auto_encoder.trainable_variables))\n",
        "\n",
        "    pre_acc \u003d ae_accuracy(x, x_hat)\n",
        "\n",
        "    return tf.reduce_mean(pre_loss), pre_acc\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "source": [
        "def pretrain_embd(X, idxs, encoder, decoder, auto_encoder, pre_optimizer, first_order, alpha):\n",
        "    np.random.shuffle(idxs)\n",
        "    PRE_EPOCHS \u003d 300\n",
        "    Batch_size \u003d 50\n",
        "\n",
        "    for epoch in range(PRE_EPOCHS):\n",
        "\n",
        "        epoch_losses \u003d []\n",
        "        epoch_acc \u003d []\n",
        "\n",
        "        for batch_idx in range(0, len(idxs), Batch_size):\n",
        "            selected_idxs \u003d idxs[batch_idx: batch_idx + Batch_size]\n",
        "            adjacency_batch \u003d X[selected_idxs, :]\n",
        "\n",
        "            loss, accuracy \u003d pretrain_step_embd(X, tf.cast(adjacency_batch, tf.float32), encoder, decoder, auto_encoder,\n",
        "                                                pre_optimizer, first_order, alpha)\n",
        "\n",
        "            epoch_losses.append(loss)\n",
        "            epoch_acc.append(accuracy)\n",
        "\n",
        "\n",
        "#     if epoch % 50 \u003d\u003d 0:\n",
        "#        print(f\"Loss is {np.array(epoch_losses).mean()} and accuracy is {np.array(epoch_acc).mean()}\")"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Discriminator Model"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Architecture"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "source": [
        "def build_discriminator(embedding_size):\n",
        "    model \u003d Sequential()\n",
        "\n",
        "    # The input layer\n",
        "    model.add(Input(shape\u003d(embedding_size,)))\n",
        "\n",
        "    # The first hidden layer\n",
        "    model.add(Dense(25, activation\u003d\u0027relu\u0027))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    # The second layer\n",
        "    model.add(Dense(15, activation\u003d\u0027relu\u0027))\n",
        "    model.add(Dropout(0.20))\n",
        "\n",
        "    # The third layer\n",
        "    model.add(Dense(6, activation\u003d\u0027relu\u0027))\n",
        "    model.add(Dropout(0.20))\n",
        "\n",
        "    model.add(Dense(1, activation\u003d\u0027sigmoid\u0027))\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "source": [
        "def disc_loss_function(d_z0, d_z1):\n",
        "    loss_zero \u003d tf.keras.losses.binary_crossentropy(tf.zeros_like(d_z0), d_z0)\n",
        "    loss_one \u003d tf.keras.losses.binary_crossentropy(tf.ones_like(d_z1), d_z1)\n",
        "\n",
        "    return tf.cast(loss_zero, tf.float32) + tf.cast(loss_one, tf.float32)\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Joint Training in a step"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "source": [
        "def train_step(x10, x11, x20, x21, encoder, decoder, auto_encoder, discriminator1, discriminator2, ae_optimizer, \n",
        "               disc1_optimizer, disc2_optimizer):\n",
        "    \n",
        "    with tf.GradientTape() as ae_tape, tf.GradientTape() as disc1_tape, tf.GradientTape() as disc2_tape:\n",
        "        z10 \u003d encoder(x10, training\u003dTrue)\n",
        "        z11 \u003d encoder(x11, training\u003dTrue)\n",
        "        \n",
        "        z20 \u003d encoder(x20, training\u003dTrue)\n",
        "        z21 \u003d encoder(x21, training\u003dTrue)\n",
        "\n",
        "        d_z10 \u003d discriminator1(z10, training\u003dTrue)\n",
        "        d_z11 \u003d discriminator1(z11, training\u003dTrue)\n",
        "        \n",
        "        d_z20 \u003d discriminator2(z20, training\u003dTrue)\n",
        "        d_z21 \u003d discriminator2(z21, training\u003dTrue)\n",
        "\n",
        "        x10_hat \u003d decoder(z10, training\u003dTrue)\n",
        "        x11_hat \u003d decoder(z11, training\u003dTrue)   \n",
        "        \n",
        "        x20_hat \u003d decoder(z20, training\u003dTrue)\n",
        "        x21_hat \u003d decoder(z21, training\u003dTrue)\n",
        "\n",
        "        ae_loss \u003d ae_adversarial_loss(tf.concat([x10, x11], 0), tf.concat([x10_hat, x11_hat], 0), d_z10, d_z11,\n",
        "                                      tf.concat([x20, x21], 0), tf.concat([x20_hat, x21_hat], 0), d_z20, d_z21,)\n",
        "        disc_loss_1 \u003d disc_loss_function(d_z10, d_z11)\n",
        "        disc_loss_2 \u003d disc_loss_function(d_z20, d_z21)\n",
        "\n",
        "    gradients_ae \u003d ae_tape.gradient(ae_loss, auto_encoder.trainable_variables)\n",
        "    gradients_disc_1 \u003d disc1_tape.gradient(disc_loss_1, discriminator1.trainable_variables)\n",
        "    gradients_disc_2 \u003d disc2_tape.gradient(disc_loss_2, discriminator2.trainable_variables)\n",
        "\n",
        "    ae_optimizer.apply_gradients(zip(gradients_ae, auto_encoder.trainable_variables))\n",
        "    disc1_optimizer.apply_gradients(zip(gradients_disc_1, discriminator1.trainable_variables))\n",
        "    disc2_optimizer.apply_gradients(zip(gradients_disc_2, discriminator2.trainable_variables))\n",
        "\n",
        "    ae_acc \u003d ae_accuracy(tf.concat([x10, x11], 0), tf.concat([x10_hat, x11_hat], 0))\n",
        "\n",
        "    return tf.reduce_mean(ae_loss), ae_acc, tf.reduce_mean(disc_loss_1) + tf.reduce_mean(disc_loss_2)\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pretrain Step for Discriminator"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "source": [
        "def pretrain_step_disc(x0, x1, encoder, discriminator, disc_pre_optimizer):\n",
        "    z0 \u003d encoder(x0)\n",
        "    z1 \u003d encoder(x1)\n",
        "\n",
        "    with tf.GradientTape() as disc_tape_sep:\n",
        "        d_z0 \u003d discriminator(z0, training\u003dTrue)\n",
        "        d_z1 \u003d discriminator(z1, training\u003dTrue)\n",
        "\n",
        "        disc_loss \u003d disc_loss_function(d_z0, d_z1)\n",
        "\n",
        "    gradients_disc \u003d disc_tape_sep.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    disc_pre_optimizer.apply_gradients(zip(gradients_disc, discriminator.trainable_variables))\n",
        "\n",
        "    return tf.reduce_mean(disc_loss)\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pretraining the discriminator"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "source": [
        "def pretrain_disc(X, idxs_zeros, idxs_ones, encoder, discriminator, disc_pre_optimizer):\n",
        "    EPOCHS \u003d 40\n",
        "\n",
        "    np.random.shuffle(idxs_zeros)\n",
        "    np.random.shuffle(idxs_ones)\n",
        "    Batch_size \u003d 50\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        for batch_idx in range(0, len(idxs_ones), Batch_size):\n",
        "            selected_zeros \u003d idxs_zeros[batch_idx: batch_idx + Batch_size]\n",
        "            selected_ones \u003d idxs_ones[batch_idx: batch_idx + Batch_size]\n",
        "\n",
        "            x0 \u003d X[selected_zeros]\n",
        "            x1 \u003d X[selected_ones]\n",
        "\n",
        "            pretrain_step_disc(x0, x1, encoder, discriminator, disc_pre_optimizer)\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The Train Loop"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "source": [
        "def adversarial_train(idxs1_zeros, idxs1_ones, idxs2_zeros, idxs2_ones, encoder, decoder, auto_encoder, discriminator1, \n",
        "                      discriminator2, ae_optimizer, disc1_optimizer, disc2_optimizer):\n",
        "    EPOCHS \u003d 500\n",
        "\n",
        "    np.random.shuffle(idxs1_zeros)\n",
        "    np.random.shuffle(idxs1_ones)\n",
        "    np.random.shuffle(idxs2_zeros)\n",
        "    np.random.shuffle(idxs2_ones)\n",
        "\n",
        "    Batch_size \u003d 50\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        for batch_idx in range(0, len(idxs1_ones), Batch_size):\n",
        "            selected_zeros1 \u003d idxs1_zeros[batch_idx: batch_idx + Batch_size]\n",
        "            selected_ones1 \u003d idxs1_ones[batch_idx: batch_idx + Batch_size]\n",
        "            selected_zeros2 \u003d idxs2_zeros[batch_idx: batch_idx + Batch_size]\n",
        "            selected_ones2 \u003d idxs2_ones[batch_idx: batch_idx + Batch_size]\n",
        "\n",
        "            x10 \u003d X[selected_zeros1]\n",
        "            x11 \u003d X[selected_ones1]\n",
        "            x20 \u003d X[selected_zeros2]\n",
        "            x21 \u003d X[selected_ones2]\n",
        "\n",
        "            ### Joint Training\n",
        "            train_step(tf.cast(x10, tf.float32), tf.cast(x11, tf.float32), tf.cast(x20, tf.float32), tf.cast(x21, tf.float32),\n",
        "                       encoder, decoder, auto_encoder, discriminator1, discriminator2, ae_optimizer, disc1_optimizer, \n",
        "                       disc2_optimizer)\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Piciking the Seeds Using Embedding"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "source": [
        "def get_seeds(N_CLUS, embedding, nodes, labels, nodes_zero, nodes_one, strategy, n_seeds):\n",
        "    \u0027\u0027\u0027\n",
        "    stratgey can be random, nearest, fair, re-cluster, fair_re-cluster\n",
        "    \u0027\u0027\u0027\n",
        "\n",
        "    model \u003d KMeans(n_clusters\u003dN_CLUS)\n",
        "    model.fit(embedding)\n",
        "\n",
        "    cluster_number \u003d model.labels_\n",
        "    centers \u003d model.cluster_centers_\n",
        "\n",
        "    seed_ids \u003d [[] for i in range(N_CLUS)]\n",
        "\n",
        "    for i in range(N_CLUS):\n",
        "\n",
        "        if strategy \u003d\u003d \u0027nearest\u0027:\n",
        "            sorted_distance \u003d np.array(sorted(\n",
        "                [[np.sqrt(np.sum(np.power(centers[i] - embedding[j], 2))), j] for j in range(len(embedding)) if\n",
        "                 i \u003d\u003d cluster_number[j]]))\n",
        "            seed_ids[i].extend(list(sorted_distance[:n_seeds, 1]))\n",
        "\n",
        "\n",
        "        elif strategy \u003d\u003d \u0027re-cluster\u0027:\n",
        "            temp \u003d []\n",
        "            sorted_distance \u003d np.array(sorted(\n",
        "                [[np.sqrt(np.sum(np.power(centers[i] - embedding[j], 2))), j] for j in range(len(embedding)) if\n",
        "                 i \u003d\u003d cluster_number[j]]))\n",
        "            temp.extend(list(sorted_distance[:n_seeds, 1]))\n",
        "\n",
        "            portion_zero \u003d 0\n",
        "            portion_one \u003d 0\n",
        "\n",
        "            for num in temp:\n",
        "                if num in nodes_zero:\n",
        "                    portion_zero +\u003d 1\n",
        "                elif num in nodes_one:\n",
        "                    portion_one +\u003d 1\n",
        "\n",
        "            zero_in_clus \u003d embedding[np.logical_and(cluster_number \u003d\u003d i, labels \u003d\u003d 0)]\n",
        "            zero_inds \u003d nodes[np.logical_and(cluster_number \u003d\u003d i, labels \u003d\u003d 0)]\n",
        "\n",
        "            one_in_clus \u003d embedding[np.logical_and(cluster_number \u003d\u003d i, labels \u003d\u003d 1)]\n",
        "            one_inds \u003d nodes[np.logical_and(cluster_number \u003d\u003d i, labels \u003d\u003d 1)]\n",
        "\n",
        "            added_to_zero \u003d 0\n",
        "            if len(zero_in_clus) !\u003d 0:\n",
        "                model_on_zero \u003d KMeans(n_clusters\u003d1)\n",
        "                model_on_zero.fit(zero_in_clus)\n",
        "                center_zero \u003d model_on_zero.cluster_centers_\n",
        "\n",
        "                sorted_distance_zero \u003d np.array(sorted(\n",
        "                    [[np.sqrt(np.sum(np.power(center_zero - zero_in_clus[j], 2))), j] for j in\n",
        "                     range(len(zero_in_clus))]))\n",
        "                seed_ids[i].extend([zero_inds[int(i)] for i in sorted_distance_zero[:portion_zero, 1]])\n",
        "\n",
        "                added_to_zero \u003d len(seed_ids[i])\n",
        "                assert added_to_zero \u003d\u003d portion_zero\n",
        "\n",
        "            added_to_one \u003d 0\n",
        "            if len(one_in_clus) !\u003d 0:\n",
        "                model_on_one \u003d KMeans(n_clusters\u003d1)\n",
        "                model_on_one.fit(one_in_clus)\n",
        "                center_one \u003d model_on_one.cluster_centers_\n",
        "\n",
        "                sorted_distance_one \u003d np.array(sorted(\n",
        "                    [[np.sqrt(np.sum(np.power(center_one - one_in_clus[j], 2))), j] for j in range(len(one_in_clus))]))\n",
        "                seed_ids[i].extend([one_inds[int(i)] for i in sorted_distance_one[:portion_one, 1]])\n",
        "\n",
        "                added_to_one \u003d len(seed_ids[i]) - added_to_zero\n",
        "                assert added_to_one \u003d\u003d portion_one\n",
        "\n",
        "            assert n_seeds \u003d\u003d added_to_zero + added_to_one\n",
        "            assert len(seed_ids[i]) \u003d\u003d n_seeds\n",
        "\n",
        "    return np.reshape(seed_ids, newshape\u003d(-1,))\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The IC algorithm"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "source": [
        "def IC(G, seeds, imp_prob, recover_prob\u003d0, remove\u003d0):\n",
        "    impressed \u003d []\n",
        "    removed \u003d []\n",
        "    front \u003d list(seeds[:])\n",
        "\n",
        "    while front:\n",
        "        impressed.extend(front)\n",
        "        impressed \u003d np.array(impressed)\n",
        "\n",
        "        if recover_prob !\u003d 0:\n",
        "\n",
        "            random_draws \u003d np.random.uniform(size\u003dlen(impressed))\n",
        "\n",
        "            if remove:\n",
        "                removed.extend(impressed[random_draws \u003c recover_prob])\n",
        "                removed \u003d list(set(removed))\n",
        "\n",
        "            impressed \u003d impressed[random_draws \u003e\u003d recover_prob]\n",
        "\n",
        "        impressed \u003d list(impressed)\n",
        "        new_front \u003d []\n",
        "\n",
        "        for node in front:\n",
        "\n",
        "            neighbours \u003d list(G.neighbors(node))\n",
        "\n",
        "            for neigh in neighbours:\n",
        "\n",
        "                expr_prob \u003d np.random.uniform(size\u003d1)[0]\n",
        "                if expr_prob \u003c imp_prob and not (neigh in impressed) and not (neigh in new_front) and not (\n",
        "                        neigh in removed):\n",
        "                    new_front.append(neigh)\n",
        "\n",
        "        front \u003d new_front[:]\n",
        "\n",
        "    impressed \u003d np.reshape(np.array(impressed), newshape\u003d(-1,))\n",
        "\n",
        "    return impressed\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Repeated IC"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "source": "def repeated_IC(G, seeds, seed_type, n_expr, imp_prob):\n    zeros_count1 \u003d []\n    ones_count1 \u003d []\n    zeros_count2 \u003d []\n    ones_count2 \u003d []\n    total_count \u003d []\n\n    for i in range(n_expr):\n        impressed \u003d IC(G, seeds, imp_prob)\n        total_count.append(len(impressed))\n\n        count_zeros1 \u003d 0\n        count_ones1 \u003d 0\n        count_zeros2 \u003d 0\n        count_ones2 \u003d 0\n\n        for imp in impressed:\n            if imp in attr1_zero:\n                count_zeros1 +\u003d 1\n            elif imp in attr1_one:\n                count_ones1 +\u003d 1\n            if imp in attr2_zero:\n                count_zeros2 +\u003d 1\n            elif imp in attr2_one:\n                count_ones2 +\u003d 1\n\n        zeros_count1.append(count_zeros1)\n        ones_count1.append(count_ones1)\n        zeros_count2.append(count_zeros2)\n        ones_count2.append(count_ones2)\n\n    total_imp \u003d np.round(np.mean(total_count), 2)\n    total_fraction \u003d np.round(total_imp / len(G.nodes()), 3)\n\n    fraction_zero1 \u003d np.round(np.mean(zeros_count1) / len(attr1_zero), 3)\n    fraction_one1 \u003d np.round(np.mean(ones_count1) / len(attr1_one), 3)\n    \n    fraction_zero2 \u003d np.round(np.mean(zeros_count2) / len(attr2_zero), 3)\n    fraction_one2 \u003d np.round(np.mean(ones_count2) / len(attr2_one), 3)\n\n    return total_imp, total_fraction, fraction_zero1, fraction_one1, fraction_zero2, fraction_one2\n",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "source": "def get_IC_influenced(G, seeds, n_expr, imp_prob):\n    zeros_count1 \u003d []\n    ones_count1 \u003d []\n    zeros_count2 \u003d []\n    ones_count2 \u003d []\n    total_count \u003d []\n\n    for i in range(n_expr):\n        impressed \u003d IC(G, seeds, imp_prob)\n        total_count.append(len(impressed))\n\n        count_zeros1 \u003d 0\n        count_ones1 \u003d 0\n        count_zeros2 \u003d 0\n        count_ones2 \u003d 0\n\n        for imp in impressed:\n            if imp in attr1_zero:\n                count_zeros1 +\u003d 1\n            elif imp in attr1_one:\n                count_ones1 +\u003d 1\n            if imp in attr2_zero:\n                count_zeros2 +\u003d 1\n            elif imp in attr2_one:\n                count_ones2 +\u003d 1\n\n        zeros_count1.append(count_zeros1)\n        ones_count1.append(count_ones1)\n        zeros_count2.append(count_zeros2)\n        ones_count2.append(count_ones2)\n\n    return np.array(total_count, count_zeros1, count_ones1)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Loading the real graph",
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "source": [
        "def get_graph_real():\n",
        "    graph_df \u003d pd.read_csv(\u0027edges.txt\u0027, sep\u003d\"\\t\", header\u003dNone)\n",
        "    graph_df.columns \u003d [\u0027s\u0027, \u0027t\u0027]\n",
        "\n",
        "    attr_df \u003d pd.read_csv(\u0027attr.txt\u0027, sep\u003d\"\\t\", header\u003dNone)\n",
        "    attr_df.columns \u003d [\u0027id\u0027, \u0027College\u0027, \u0027Age\u0027, \u0027Major\u0027]\n",
        "\n",
        "    edges \u003d []\n",
        "\n",
        "    for index, row in graph_df.iterrows():\n",
        "        edge_cur \u003d (row.s, row.t)\n",
        "\n",
        "        edges.append(edge_cur)\n",
        "\n",
        "    input_G \u003d nx.from_edgelist(edges)\n",
        "\n",
        "    extra_nodes \u003d []\n",
        "    for index, row in attr_df.iterrows():\n",
        "        if row.Age \u003e 20:\n",
        "            extra_nodes.append(row.id)\n",
        "    \n",
        "    unfrozen_G \u003d nx.Graph(input_G)\n",
        "    \n",
        "    for node in input_G.nodes():\n",
        "        if node in extra_nodes:\n",
        "            unfrozen_G.remove_node(node)   \n",
        "    \n",
        "    X \u003d nx.to_numpy_matrix(unfrozen_G)\n",
        "    G \u003d nx.from_numpy_matrix(X)\n",
        "\n",
        "    return G, X, unfrozen_G\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "source": [
        "def get_nodes_labels_real():\n",
        "    \n",
        "    # 1. Building the graph.\n",
        "    graph_df \u003d pd.read_csv(\u0027edges.txt\u0027, sep\u003d\"\\t\", header\u003dNone)\n",
        "    graph_df.columns \u003d [\u0027s\u0027, \u0027t\u0027]\n",
        "    \n",
        "    edges \u003d []\n",
        "    for index, row in graph_df.iterrows():\n",
        "        edge_cur \u003d (row.s, row.t)\n",
        "\n",
        "        edges.append(edge_cur)\n",
        "        \n",
        "    input_G \u003d nx.from_edgelist(edges)\n",
        "    unfrozen_G \u003d nx.Graph(input_G)\n",
        "    nodes_G \u003d list(input_G.nodes())\n",
        "\n",
        "    # 2. Fetching the attributes\n",
        "    data \u003d pd.read_csv(\u0027attr.txt\u0027, sep\u003d\"\\t\", header\u003dNone)\n",
        "    \n",
        "    # Major takes a value between 1-60\n",
        "    data.columns \u003d [\u0027id\u0027, \u0027College\u0027, \u0027Age\u0027, \u0027Major\u0027]\n",
        "    \n",
        "    extra_nodes \u003d []\n",
        "    attr1_labels \u003d {}\n",
        "    attr2_labels \u003d {}\n",
        "\n",
        "    count_l20 \u003d 0\n",
        "    count_e20 \u003d 0\n",
        "    count_g20 \u003d 0\n",
        "\n",
        "    for index, row in data.iterrows():\n",
        "        # Grouping based on the first attribute\n",
        "        # Ages take a value between 18-24\n",
        "        if row.Age \u003c 20 and row.id in nodes_G:\n",
        "            attr1_labels[row.id] \u003d 0\n",
        "            count_l20 +\u003d 1\n",
        "        elif row.Age \u003d\u003d 20 and row.id in nodes_G:\n",
        "            attr1_labels[row.id] \u003d 1\n",
        "            count_e20 +\u003d 1\n",
        "        elif row.Age \u003e 20 and row.id in nodes_G:\n",
        "            attr1_labels[row.id] \u003d 2\n",
        "            extra_nodes.append(row.id)\n",
        "            count_g20 +\u003d 1\n",
        "        \n",
        "        # Grouping based on the second attribute\n",
        "        # College takes a value between 1-9\n",
        "        if row.College \u003c 7 and row.id in nodes_G:\n",
        "            attr2_labels[row.id] \u003d 0\n",
        "        elif row.College \u003e\u003d 7 and row.id in nodes_G:\n",
        "            attr2_labels[row.id] \u003d 1\n",
        "    \n",
        "    # 3. Pruning the data based on the age labels (the get the same dataset as before)\n",
        "    for node in input_G.nodes():\n",
        "        if node in extra_nodes:\n",
        "            unfrozen_G.remove_node(node)\n",
        "    # Asserting the size\n",
        "    # print(len(list(unfrozen_G.nodes())))\n",
        "    # print(count_l20 + count_e20)\n",
        "    \n",
        "    labels_attr1 \u003d []\n",
        "    labels_attr2 \u003d []\n",
        "    \n",
        "    # 4. creating the list of attribute labels\n",
        "    for node in unfrozen_G.nodes():\n",
        "        labels_attr1.append(attr1_labels[node])\n",
        "        labels_attr2.append(attr2_labels[node])\n",
        "    assert len(labels_attr1) \u003d\u003d len(labels_attr2)\n",
        "    \n",
        "    # Remember that whenever you want do logical operation on a sequence, that sequence should be numpy array\n",
        "    labels_attr1 \u003d np.array(labels_attr1)\n",
        "    labels_attr2 \u003d np.array(labels_attr2)\n",
        "\n",
        "    nodes \u003d np.arange(len(unfrozen_G.nodes()))\n",
        "    \n",
        "    attr1_zero \u003d nodes[labels_attr1 \u003d\u003d 0]\n",
        "    attr1_one \u003d nodes[labels_attr1 \u003d\u003d 1]\n",
        "    \n",
        "    attr2_zero \u003d nodes[labels_attr2 \u003d\u003d 0]\n",
        "    attr2_one \u003d nodes[labels_attr2 \u003d\u003d 1]\n",
        "    \n",
        "    # Size of zero and one community of each attribute should be different\n",
        "    # print(f\u0027For attribute 1, zero: {len(attr1_zero)}, one: {len(attr1_one)}\u0027)\n",
        "    # print(f\u0027for attribute 2, zero: {len(attr2_zero)}, one: {len(attr2_one)}\u0027)\n",
        "    # \n",
        "    # # Counting the number of both zero and both one\n",
        "    # print(f\u0027The number of both zeros are: {len(nodes[np.logical_and(labels_attr1 \u003d\u003d 0, labels_attr2 \u003d\u003d 0)])}\u0027)\n",
        "    # print(f\u0027The number of both ones are: {len(nodes[np.logical_and(labels_attr1 \u003d\u003d 1, labels_attr2 \u003d\u003d 1)])}\u0027)\n",
        "\n",
        "    return attr1_zero, attr1_one, labels_attr1, attr2_zero, attr2_one, labels_attr2\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "source": [
        "def get_idxs(n, nodes_zero, nodes_one, idxs_size):\n",
        "\n",
        "    idxs_zeros \u003d nodes_zero[:]\n",
        "    idxs_ones \u003d nodes_one[:]\n",
        "    \n",
        "    rep_policy \u003d False\n",
        "    if len(nodes_zero) \u003c idxs_size:\n",
        "        if idxs_size  \u003e 2 * len(nodes_zero):\n",
        "            rep_policy \u003d True\n",
        "        zero_draws \u003d np.random.choice(nodes_zero, size\u003didxs_size - len(nodes_zero), replace\u003drep_policy)\n",
        "        idxs_zeros \u003d np.concatenate((idxs_zeros, zero_draws))\n",
        "    \n",
        "    rep_policy \u003d False\n",
        "    if len(nodes_one) \u003c idxs_size:\n",
        "        if idxs_size \u003e 2 * len(nodes_one):\n",
        "            rep_policy \u003d True\n",
        "        one_draws \u003d np.random.choice(nodes_one, size\u003didxs_size - len(nodes_one), replace\u003drep_policy)\n",
        "        idxs_ones \u003d np.concatenate((idxs_ones, one_draws))\n",
        "\n",
        "    assert len(idxs_zeros) \u003d\u003d len(idxs_ones)\n",
        "\n",
        "    return np.arange(n), idxs_zeros, idxs_ones\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "source": [
        "def print_edges(G, nodes_zero, nodes_one):\n",
        "    zero_edges \u003d 0\n",
        "    one_edges \u003d 0\n",
        "    accross_edges \u003d 0\n",
        "\n",
        "    for (v1, v2) in G.edges():\n",
        "        if v1 in nodes_zero and v2 in nodes_zero:\n",
        "            zero_edges +\u003d 1\n",
        "        elif v1 in nodes_one and v2 in nodes_one:\n",
        "            one_edges +\u003d 1\n",
        "        elif v1 in nodes_one and v2 in nodes_zero:\n",
        "            accross_edges +\u003d 1\n",
        "        elif v1 in nodes_zero and v2 in nodes_one:\n",
        "            accross_edges +\u003d 1\n",
        "\n",
        "    print(f\" edges in zero community: {zero_edges}\")\n",
        "    print(f\" edges in one community: {one_edges}\")\n",
        "    print(f\" edges across communities: {accross_edges}\")\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the experiments"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            " edges in zero community: 513\n edges in one community: 7441\n edges across communities: 1706\n",
            "WARNING:tensorflow:Layer sequential is casting an input tensor from dtype float64 to the layer\u0027s dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it\u0027s dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx(\u0027float64\u0027)`. To change just this layer, pass dtype\u003d\u0027float64\u0027 to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast\u003dFalse to the base Layer constructor.\n\n",
            "pre-training done.\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "\n",
        "embedding_size \u003d 30\n",
        "\n",
        "# Can get `with_f1` or `without_f1`\n",
        "first_order_imp \u003d \u0027no_f1\u0027\n",
        "alpha \u003d 0.05\n",
        "\n",
        "# 1. Creating the Graph and Getting the Adj Matrix\n",
        "G, X, input_G \u003d get_graph_real()\n",
        "n \u003d len(G.nodes())\n",
        "\n",
        "# 2. Getting seperate lists for seperate communities and the label for each community\n",
        "attr1_zero, attr1_one, labels_attr1, attr2_zero, attr2_one, labels_attr2 \u003d get_nodes_labels_real()\n",
        "\n",
        "idxs_size \u003d np.max([len(attr1_zero), len(attr1_one), len(attr2_zero), len(attr2_one)])\n",
        "\n",
        "print_edges(G, attr1_zero, attr1_one)\n",
        "\n",
        "# 3. Getting the idxs suitable for training.\n",
        "idxs, idxs1_zeros, idxs1_ones \u003d get_idxs(n, attr1_zero, attr1_one, idxs_size)\n",
        "_, idxs2_zeros, idxs2_ones \u003d get_idxs(n, attr2_zero, attr2_one, idxs_size)\n",
        "\n",
        "# 4. Creating the Embedder\n",
        "encoder \u003d build_encoder(embedding_size)\n",
        "decoder \u003d build_decoder(embedding_size, n)\n",
        "auto_encoder \u003d build_ae(encoder, decoder, n)\n",
        "\n",
        "# 5. Creating the Discriminator\n",
        "discriminator1 \u003d build_discriminator(embedding_size)\n",
        "discriminator2 \u003d build_discriminator(embedding_size)\n",
        "\n",
        "# 6. Pretraining the Embedder and the Discriminator\n",
        "pre_optimizer_embd \u003d tf.keras.optimizers.Adam()\n",
        "pre_optimizer_disc1 \u003d tf.keras.optimizers.Adam()\n",
        "pre_optimizer_disc2 \u003d tf.keras.optimizers.Adam()\n",
        "\n",
        "time1\u003d time.time()\n",
        "pretrain_embd(X, idxs, encoder, decoder, auto_encoder, pre_optimizer_embd, first_order_imp, alpha)\n",
        "pretrain_disc(X, idxs1_zeros, idxs1_ones, encoder, discriminator1, pre_optimizer_disc1)\n",
        "pretrain_disc(X, idxs2_zeros, idxs2_ones, encoder, discriminator2, pre_optimizer_disc2)\n",
        "# print(\u00276\u0027)\n",
        "\n",
        "# # 6-1. Get the pretrain-embeddings\n",
        "pre_embds \u003d encoder(X)\n",
        "print(\u0027pre-training done.\u0027)"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "adversarial training done.\n724.38\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "# 7. Adversarial Training\n",
        "ae_optimizer \u003d tf.keras.optimizers.Adam()\n",
        "disc1_optimizer \u003d tf.keras.optimizers.Adam()\n",
        "disc2_optimizer \u003d tf.keras.optimizers.Adam()\n",
        "\n",
        "adversarial_train(idxs1_zeros, idxs1_ones, idxs2_zeros, idxs2_ones, encoder, decoder, auto_encoder, discriminator1, \n",
        "                  discriminator2, ae_optimizer, disc1_optimizer, disc2_optimizer)\n",
        "\n",
        "# # 6-1. Get the pretrain-embeddings\n",
        "fair_embds \u003d encoder(X)\n",
        "print(\u0027adversarial training done.\u0027)\n",
        "time_spent \u003d np.round(time.time() - time1, 2)\n",
        "\n",
        "print(time_spent)"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experiments settings"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {}
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "source": [
        "N_CLUSs \u003d [4]\n",
        "\n",
        "n_seedss \u003d [1, 2, 3, 4, 5, 6, 7, 8, 10]\n",
        "# n_seedss \u003d [8]\n",
        "\n",
        "# Methods for getting the seeds can be nearest or re-cluster\n",
        "strategy \u003d \u0027re-cluster\u0027"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "source": [
        "# Loading from somewhere else\n",
        "# \n",
        "# with open(\u0027saved_models/embds_2_1.pickle\u0027, \u0027rb\u0027) as f:\n",
        "# #     _, _, fair_embds, pre_embds, _, _, _, _ \u003d pickle.load(f)\n",
        "#     G, embedding_size, fair_embds, pre_embds, idxs, labels_attr1, attr1_zero, attr1_one \u003d pickle.load(f)"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "source": [
        "# Loading the greedy seeds\n",
        "\n",
        "with open(\u0027saved_models/greedy_seeds.pickle\u0027, \u0027rb\u0027) as f:\n",
        "#     _, _, fair_embds, pre_embds, _, _, _, _ \u003d pickle.load(f)\n",
        "    greedy_seeds \u003d pickle.load(f)\n",
        "\n",
        "greedy_seeds \u003d np.array(greedy_seeds[0])\n",
        "# print(greedy_seeds)"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "[1, 8.74, 0.02, 0.004, 0.024, 0.026, 0.008]\n",
            "[2, 16.94, 0.038, 0.009, 0.047, 0.051, 0.016]\n",
            "[3, 25.62, 0.058, 0.014, 0.07, 0.073, 0.032]\n",
            "[4, 34.06, 0.077, 0.022, 0.093, 0.093, 0.05]\n",
            "[5, 40.56, 0.092, 0.025, 0.111, 0.108, 0.063]\n",
            "[6, 46.68, 0.106, 0.028, 0.128, 0.124, 0.074]\n"
          ],
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m\u003cipython-input-30-6f17bdcde4e9\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m          \u001b[1;31m# Results of the greedy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---\u003e 26\u001b[1;33m         \u001b[0mtotal_greedy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgreedy_frac\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgreedy_zero_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgreedy_one_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgreedy_zero_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgreedy_one_2\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mrepeated_IC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgreedy_seeds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseeds_cur\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\u0027greedy\u0027\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mrow_greedy\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mn_seeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_greedy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgreedy_frac\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgreedy_zero_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgreedy_one_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgreedy_zero_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgreedy_one_2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_greedy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m\u003cipython-input-20-54e57699d922\u003e\u001b[0m in \u001b[0;36mrepeated_IC\u001b[1;34m(G, seeds, seed_type, n_expr, imp_prob)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_expr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----\u003e 9\u001b[1;33m         \u001b[0mimpressed\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mIC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimp_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mtotal_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimpressed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m\u003cipython-input-19-0b50a0c8c6fa\u003e\u001b[0m in \u001b[0;36mIC\u001b[1;34m(G, seeds, imp_prob, recover_prob, remove)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mneigh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mneighbours\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---\u003e 29\u001b[1;33m                 \u001b[0mexpr_prob\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m                 if expr_prob \u003c imp_prob and not (neigh in impressed) and not (neigh in new_front) and not (\n\u001b[0;32m     31\u001b[0m                         neigh in removed):\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "source": "rows \u003d \u0027[\u0027\nfirst \u003d True\nseeds_cur \u003d 0\nfor N_CLUS in N_CLUSs:\n    for n_seeds in n_seedss:\n        # if first:\n        #     first \u003d False\n        # else:\n        #     rows +\u003d \u0027,\\n\u0027\n        # # 8. Getting the seeds for the embeddings and baselines\n        fair_seeds \u003d get_seeds(N_CLUS, fair_embds, idxs, labels_attr1, attr1_zero, attr1_one, strategy, n_seeds)\n        # pre_seeds \u003d get_seeds(N_CLUS, pre_embds, idxs, labels_attr1, attr1_zero, attr1_one, strategy, n_seeds)\n        # # \n        # # # 9. Getting the final results\n        total_fair, fair_frac, zero_fair1, one_fair1, zero_fair2, one_fair2 \u003d repeated_IC(G, fair_seeds, \u0027fair\u0027, 2000, 0.01)\n        # total_pre, pre_frac, zero_pre1, one_pre1, zero_pre2, one_pre2 \u003d repeated_IC(G, pre_seeds, \u0027pre\u0027, 2000, 0.01)\n        # # \n        # # 10. Building the current row and adding it to the rows.\n        # row_fair \u003d [embedding_size, N_CLUS, n_seeds, total_fair, fair_frac, zero_fair1, one_fair1, zero_fair2, one_fair2, \u0027\\\u0027\u0027 + strategy + \u0027\\\u0027\u0027]\n        # row_pre \u003d [embedding_size, N_CLUS, n_seeds, total_pre, pre_frac, zero_pre1, one_pre1, zero_pre2, one_pre2, \u0027\\\u0027\u0027 + strategy + \u0027\\\u0027\u0027]\n        # print(row_fair)\n        # print(row_pre)\n        # print(\u0027\u0027)\n        \n         # Results of the greedy\n        total_greedy, greedy_frac, greedy_zero_1, greedy_one_1, greedy_zero_2, greedy_one_2 \u003d repeated_IC(G, np.reshape(greedy_seeds[seeds_cur], newshape\u003d(-1,)), \u0027greedy\u0027, 2000, 0.01)\n        row_greedy \u003d [n_seeds, total_greedy, greedy_frac, greedy_zero_1, greedy_one_1, greedy_zero_2, greedy_one_2]\n        print(row_greedy)\n        seeds_cur +\u003d 1\n\n        # rows +\u003d \u0027[\u0027 + \u0027, \u0027.join(map(str, row)) + \u0027]\u0027\n\n# rows +\u003d \u0027]\u0027\n# \n# print(rows)",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\u0027embds_2_5.pickle\u0027, \u0027wb\u0027) as f:\n",
        "    pickle.dump([G, embedding_size, fair_embds, pre_embds, idxs, labels_attr1, attr1_zero, attr1_one], f)\n",
        "\n",
        "print(\u0027Saved\u0027)\n",
        "\n",
        "# %%\n",
        "# G, X, input_G \u003d get_graph_real()\n",
        "# n \u003d len(G.nodes())\n",
        "\n",
        "# 2. Getting seperate lists for seperate communities and the label for each community\n",
        "# nodes_zero, nodes_one, labels \u003d get_nodes_labels_real()"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "saved_new\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "with open(\u0027labels_nodes.pickle\u0027, \u0027wb\u0027) as f:\n    pickle.dump([G, idxs, labels_attr1, attr1_zero, attr1_one, labels_attr2, attr2_zero, attr2_one], f)\n\nprint(\u0027saved_new\u0027)\n\n",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "[[[0, 1, 2, 3], [0, 1, 2, 3, 4, 5, 6, 7], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]]]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# Doing the t-test\n\n# with open(\u0027saved_models/greedy_seeds.pickle\u0027, \u0027rb\u0027) as f:\n# #     _, _, fair_embds, pre_embds, _, _, _, _ \u003d pickle.load(f)\n#     greedy_seeds \u003d pickle.load(f)\n# print(greedy_seeds)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "source": "import scipy as sc",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "outputs": [],
      "source": "with open(\u0027saved_models/embds_2_1.pickle\u0027, \u0027rb\u0027) as f:\n#     _, _, fair_embds, pre_embds, _, _, _, _ \u003d pickle.load(f)\n    [G, embedding_size, fair_embds, pre_embds, idxs, labels_attr1, attr1_zero, attr1_one] \u003d pickle.load(f)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "outputs": [
        {
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m\u003cipython-input-50-9fbe25faee67\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_seeds\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mn_seedss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[0mN_CLUS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----\u003e 9\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfair_embds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;31m# fair_seeds \u003d get_seeds(N_CLUS, fair_embds, idxs, labels_attr1, attr1_zero, attr1_one, strategy, n_seeds)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# fair_records \u003d get_IC_influenced(G, fair_seeds, 2000, 0.01)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    969\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--\u003e 971\u001b[1;33m                 return_n_iter\u003dTrue)\n\u001b[0m\u001b[0;32m    972\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[1;34m(X, n_clusters, sample_weight, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[0morder\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[1;34m\"C\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcopy_x\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     X \u003d check_array(X, accept_sparse\u003d\u0027csr\u0027, dtype\u003d[np.float64, np.float32],\n\u001b[1;32m--\u003e 311\u001b[1;33m                     order\u003dorder, copy\u003dcopy_x)\n\u001b[0m\u001b[0;32m    312\u001b[0m     \u001b[1;31m# verify that the number of samples given is larger than k\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m\u003c\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\u0027error\u0027\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--\u003e 527\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
            "\u001b[1;32mc:\\python\\python37\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--\u003e 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
          ],
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence.",
          "output_type": "error"
        }
      ],
      "source": "p_values \u003d []\nN_CLUSs \u003d [4]\nn_seedss \u003d [1, 2, 3, 4, 5, 6, 7, 8, 10]\nstrategy \u003d \u0027re-cluster\u0027\n\nfor N_CLUS in N_CLUSs:\n    for n_seeds in n_seedss:\n        model \u003d KMeans(n_clusters\u003dN_CLUS)\n        model.fit(fair_embds)\n        # fair_seeds \u003d get_seeds(N_CLUS, fair_embds, idxs, labels_attr1, attr1_zero, attr1_one, strategy, n_seeds)\n        # fair_records \u003d get_IC_influenced(G, fair_seeds, 2000, 0.01)\n        print(n_seeds)\n        \n        \n        pre_seeds \u003d get_seeds(N_CLUS, pre_embds, idxs, labels_attr1, attr1_zero, attr1_one, strategy, n_seeds)\n        pre_records \u003d get_IC_influenced(G, pre_seeds, 2000, 0.1)\n        \n        t_stat, p_value \u003d sc.stats.ttest_ind(fair_seeds, pre_seeds)\n        p_values.append(p_value)\n\nprint(p_values)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}